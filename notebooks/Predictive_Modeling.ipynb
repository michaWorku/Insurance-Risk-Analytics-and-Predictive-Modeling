{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d346440f",
   "metadata": {},
   "source": [
    "# **Predictive Modeling for Insurance Risk and Premium Optimization**\n",
    "\n",
    "This notebook implements Task 4 of the challenge: \"Build and evaluate predictive models that form the core of a dynamic, risk-based pricing system.\" We will focus on two primary modeling goals: predicting `Claim Severity` and exploring approaches for `Premium Optimization`.\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Setup and Data Loading](#1-setup-and-data-loading)\n",
    "2. [Data Preparation](#2-data-preparation)\n",
    "    - 2.1. Initial Data Loading and Cleaning\n",
    "    - 2.2. Feature Engineering\n",
    "    - 2.3. Encoding Categorical Data & Scaling Numerical Data\n",
    "    - 2.4. Train-Test Split\n",
    "3. [Modeling Goal 1: Claim Severity Prediction (Risk Model)](#3-modeling-goal-1-claim-severity-prediction-risk-model)\n",
    "    - 3.1. Model Building & Training\n",
    "    - 3.2. Model Evaluation\n",
    "    - 3.3. Model Interpretability (SHAP & LIME)\n",
    "4. [Modeling Goal 2: Premium Optimization (Pricing Framework)](#4-modeling-goal-2-premium-optimization-pricing-framework)\n",
    "    - 4.1. Conceptual Framework: Risk-Based Premium\n",
    "    - 4.2. Model Building: Probability of Claim (Classification Model)\n",
    "    - 4.3. Model Evaluation: Probability of Claim\n",
    "    - 4.4. Model Interpretability (SHAP & LIME) for Probability Model\n",
    "    - 4.5. Simplified Premium Prediction (Direct Regression on TotalPremium)\n",
    "5. [Overall Model Comparison and Interpretation](#5-overall-model-comparison-and-interpretation)\n",
    "6. [Conclusion and Business Recommendations](#6-conclusion-and-business-recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78664d0a",
   "metadata": {},
   "source": [
    "## **1. Setup and Data Loading**\n",
    "\n",
    "We begin by importing all necessary libraries and our custom modular functions for data loading, preprocessing, and modeling. We'll load the processed data from Task 1, which should be available in `data/processed/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9092801",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c082c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97376b96",
   "metadata": {},
   "source": [
    "### Setp plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d138250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings from libraries like shap for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='shap')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='shap')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['font.family'] = 'Inter'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62594c1a",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7545f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path to enable importing modular scripts\n",
    "import sys\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import data handling utilities\n",
    "# from src.utils.data_loader import load_data # Now handled by DataPreprocessor.load_and_clean_data\n",
    "\n",
    "# Import new data preparation utilities\n",
    "from src.utils.data_preparation.preprocessor import DataPreprocessor\n",
    "from src.utils.data_preparation.feature_engineer import create_time_features, create_risk_ratio_features, create_vehicle_age_feature\n",
    "\n",
    "# Import modeling utilities\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "from src.models.linear_regression_strategy import LinearRegressionStrategy\n",
    "from src.models.random_forest_strategy import RandomForestStrategy\n",
    "from src.models.decision_tree_strategy import DecisionTreeStrategy # New\n",
    "from src.models.xgboost_strategy import XGBoostStrategy\n",
    "from src.models.model_evaluator import evaluate_regression_model, evaluate_classification_model\n",
    "from src.models.model_interpreter import ModelInterpreter\n",
    "\n",
    "# Import metrics calculator from  for HasClaim and Margin\n",
    "from src.utils.hypothesis_testing.metrics_calculator import calculate_claim_frequency, calculate_margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d842b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Load and Preprocss Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b0a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load and clean data from: /home/micha/Downloads/course/10-accademy/week-3/Insurance-Risk-Analytics-and-Predictive-Modeling/data/raw/temp_extracted_data/MachineLearningRating_v3.txt\n",
      "Successfully loaded data from 'MachineLearningRating_v3.txt' using specified delimiter '|'. Shape: (1000098, 52)\n",
      "Removed 0 duplicate rows.\n",
      "Data loaded and duplicates removed. Current shape: (1000098, 52)\n",
      "\n",
      "DataFrame shape after loading and initial cleaning: (1000098, 52)\n",
      "\n",
      "Initial DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000098 entries, 0 to 1000097\n",
      "Data columns (total 52 columns):\n",
      " #   Column                    Non-Null Count    Dtype  \n",
      "---  ------                    --------------    -----  \n",
      " 0   UnderwrittenCoverID       1000098 non-null  int64  \n",
      " 1   PolicyID                  1000098 non-null  int64  \n",
      " 2   TransactionMonth          1000098 non-null  object \n",
      " 3   IsVATRegistered           1000098 non-null  bool   \n",
      " 4   Citizenship               1000098 non-null  object \n",
      " 5   LegalType                 1000098 non-null  object \n",
      " 6   Title                     1000098 non-null  object \n",
      " 7   Language                  1000098 non-null  object \n",
      " 8   Bank                      854137 non-null   object \n",
      " 9   AccountType               959866 non-null   object \n",
      " 10  MaritalStatus             991839 non-null   object \n",
      " 11  Gender                    990562 non-null   object \n",
      " 12  Country                   1000098 non-null  object \n",
      " 13  Province                  1000098 non-null  object \n",
      " 14  PostalCode                1000098 non-null  int64  \n",
      " 15  MainCrestaZone            1000098 non-null  object \n",
      " 16  SubCrestaZone             1000098 non-null  object \n",
      " 17  ItemType                  1000098 non-null  object \n",
      " 18  mmcode                    999546 non-null   float64\n",
      " 19  VehicleType               999546 non-null   object \n",
      " 20  RegistrationYear          1000098 non-null  int64  \n",
      " 21  make                      999546 non-null   object \n",
      " 22  Model                     999546 non-null   object \n",
      " 23  Cylinders                 999546 non-null   float64\n",
      " 24  cubiccapacity             999546 non-null   float64\n",
      " 25  kilowatts                 999546 non-null   float64\n",
      " 26  bodytype                  999546 non-null   object \n",
      " 27  NumberOfDoors             999546 non-null   float64\n",
      " 28  VehicleIntroDate          999546 non-null   object \n",
      " 29  CustomValueEstimate       220456 non-null   float64\n",
      " 30  AlarmImmobiliser          1000098 non-null  object \n",
      " 31  TrackingDevice            1000098 non-null  object \n",
      " 32  CapitalOutstanding        1000096 non-null  object \n",
      " 33  NewVehicle                846803 non-null   object \n",
      " 34  WrittenOff                358197 non-null   object \n",
      " 35  Rebuilt                   358197 non-null   object \n",
      " 36  Converted                 358197 non-null   object \n",
      " 37  CrossBorder               698 non-null      object \n",
      " 38  NumberOfVehiclesInFleet   0 non-null        float64\n",
      " 39  SumInsured                1000098 non-null  float64\n",
      " 40  TermFrequency             1000098 non-null  object \n",
      " 41  CalculatedPremiumPerTerm  1000098 non-null  float64\n",
      " 42  ExcessSelected            1000098 non-null  object \n",
      " 43  CoverCategory             1000098 non-null  object \n",
      " 44  CoverType                 1000098 non-null  object \n",
      " 45  CoverGroup                1000098 non-null  object \n",
      " 46  Section                   1000098 non-null  object \n",
      " 47  Product                   1000098 non-null  object \n",
      " 48  StatutoryClass            1000098 non-null  object \n",
      " 49  StatutoryRiskType         1000098 non-null  object \n",
      " 50  TotalPremium              1000098 non-null  float64\n",
      " 51  TotalClaims               1000098 non-null  float64\n",
      "dtypes: bool(1), float64(11), int64(4), object(36)\n",
      "memory usage: 390.1+ MB\n",
      "Calculated 'HasClaim' (Claim Frequency indicator) column.\n",
      "Calculated 'Margin' column.\n",
      "\n",
      "DataFrame head after loading, initial cleaning, and initial metric calculation:\n",
      "   UnderwrittenCoverID  PolicyID     TransactionMonth  IsVATRegistered  \\\n",
      "0               145249     12827  2015-03-01 00:00:00             True   \n",
      "1               145249     12827  2015-05-01 00:00:00             True   \n",
      "2               145249     12827  2015-07-01 00:00:00             True   \n",
      "3               145255     12827  2015-05-01 00:00:00             True   \n",
      "4               145255     12827  2015-07-01 00:00:00             True   \n",
      "\n",
      "  Citizenship          LegalType Title Language                 Bank  \\\n",
      "0              Close Corporation    Mr  English  First National Bank   \n",
      "1              Close Corporation    Mr  English  First National Bank   \n",
      "2              Close Corporation    Mr  English  First National Bank   \n",
      "3              Close Corporation    Mr  English  First National Bank   \n",
      "4              Close Corporation    Mr  English  First National Bank   \n",
      "\n",
      "       AccountType  ...   CoverType            CoverGroup  \\\n",
      "0  Current account  ...  Windscreen  Comprehensive - Taxi   \n",
      "1  Current account  ...  Windscreen  Comprehensive - Taxi   \n",
      "2  Current account  ...  Windscreen  Comprehensive - Taxi   \n",
      "3  Current account  ...  Own Damage  Comprehensive - Taxi   \n",
      "4  Current account  ...  Own Damage  Comprehensive - Taxi   \n",
      "\n",
      "               Section                          Product  StatutoryClass  \\\n",
      "0  Motor Comprehensive  Mobility Metered Taxis: Monthly      Commercial   \n",
      "1  Motor Comprehensive  Mobility Metered Taxis: Monthly      Commercial   \n",
      "2  Motor Comprehensive  Mobility Metered Taxis: Monthly      Commercial   \n",
      "3  Motor Comprehensive  Mobility Metered Taxis: Monthly      Commercial   \n",
      "4  Motor Comprehensive  Mobility Metered Taxis: Monthly      Commercial   \n",
      "\n",
      "  StatutoryRiskType TotalPremium TotalClaims  HasClaim      Margin  \n",
      "0     IFRS Constant    21.929825         0.0         0   21.929825  \n",
      "1     IFRS Constant    21.929825         0.0         0   21.929825  \n",
      "2     IFRS Constant     0.000000         0.0         0    0.000000  \n",
      "3     IFRS Constant   512.848070         0.0         0  512.848070  \n",
      "4     IFRS Constant     0.000000         0.0         0    0.000000  \n",
      "\n",
      "[5 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define path to the processed data file\n",
    "processed_data_path = project_root / \"data\" / \"processed\" / \"processed_insurance_data.csv\"\n",
    "raw_data_path = project_root / \"data\" / \"raw\" / \"temp_extracted_data\" / \"MachineLearningRating_v3.txt\"\n",
    "\n",
    "# Load and initially clean data using the new preprocessor method\n",
    "# print(f\"Attempting to load and clean data from: {processed_data_path}\")\n",
    "print(f\"Attempting to load and clean data from: {raw_data_path}\")\n",
    "\n",
    "# Use comma delimiter as specified\n",
    "df = DataPreprocessor.load_and_clean_data(raw_data_path, delimiter='|', file_type='txt')\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(f\"DataFrame is empty. Please ensure '{raw_data_path}' exists and is correctly formatted. \"\n",
    "                     \"This notebook expects processed data from previous tasks.\")\n",
    "\n",
    "print(f\"\\nDataFrame shape after loading and initial cleaning: {df.shape}\")\n",
    "print(\"\\nInitial DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Add 'HasClaim' and 'Margin' columns from  metrics calculator\n",
    "df = calculate_claim_frequency(df.copy())\n",
    "df = calculate_margin(df.copy())\n",
    "\n",
    "print(\"\\nDataFrame head after loading, initial cleaning, and initial metric calculation:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec95db",
   "metadata": {},
   "source": [
    "## **2. Data Preparation**\n",
    "\n",
    "This phase is critical for transforming the raw or initially processed data into a format suitable for machine learning models. It involves handling missing values, creating new informative features, encoding categorical variables, and splitting the data for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc6f13",
   "metadata": {},
   "source": [
    "### **2.1. Initial Data Loading and Cleaning**\n",
    "\n",
    "This step (already performed in section 1) ensures duplicates are removed and critical financial columns `TotalPremium`, `TotalClaims` are properly loaded and `NaN`s are filled with 0. Further imputation for other features will be handled by the `DataPreprocessor`'s pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d20592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.1. Initial Data Loading and Cleaning ---\n",
      "Initial loading and cleaning (duplicate removal, critical NaN handling) already performed in Setup section.\n",
      "\n",
      "Missing values remaining after initial load and clean:\n",
      "NumberOfVehiclesInFleet    1000098\n",
      "CrossBorder                 999400\n",
      "CustomValueEstimate         779642\n",
      "WrittenOff                  641901\n",
      "Converted                   641901\n",
      "Rebuilt                     641901\n",
      "NewVehicle                  153295\n",
      "Bank                        145961\n",
      "AccountType                  40232\n",
      "Gender                        9536\n",
      "MaritalStatus                 8259\n",
      "mmcode                         552\n",
      "VehicleType                    552\n",
      "make                           552\n",
      "VehicleIntroDate               552\n",
      "NumberOfDoors                  552\n",
      "bodytype                       552\n",
      "kilowatts                      552\n",
      "cubiccapacity                  552\n",
      "Cylinders                      552\n",
      "Model                          552\n",
      "CapitalOutstanding               2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2.1. Initial Data Loading and Cleaning ---\")\n",
    "print(\"Initial loading and cleaning (duplicate removal, critical NaN handling) already performed in Setup section.\")\n",
    "\n",
    "print(\"\\nMissing values remaining after initial load and clean:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False))\n",
    "\n",
    "# For any other non-critical numerical/categorical columns that may have NaNs,\n",
    "# the DataPreprocessor's pipeline will handle their imputation before scaling/encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff7f68",
   "metadata": {},
   "source": [
    "### **2.2. Feature Engineering**\n",
    "\n",
    "We'll create new features that might enhance model performance by capturing additional information or relationships from existing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6f9425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.2. Feature Engineering ---\n",
      "DEBUG: Initial 'TransactionMonth' dtype: object\n",
      "DEBUG: Initial 'TransactionMonth' head:\n",
      "0    2015-03-01 00:00:00\n",
      "1    2015-05-01 00:00:00\n",
      "2    2015-07-01 00:00:00\n",
      "3    2015-05-01 00:00:00\n",
      "4    2015-07-01 00:00:00\n",
      "Name: TransactionMonth, dtype: object\n",
      "Warning: 'TransactionMonth' is not datetime type. Attempting conversion for feature engineering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micha/Downloads/course/10-accademy/week-3/Insurance-Risk-Analytics-and-Predictive-Modeling/src/utils/data_preparation/feature_engineer.py:61: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  converted_dates_1 = pd.to_datetime(df_copy[date_col], errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: NaNs after inferring format: 0 / 1000098\n",
      "  Using inferred parsing result for 'TransactionMonth'.\n",
      "Created time-based features from 'TransactionMonth'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/micha/Downloads/course/10-accademy/week-3/Insurance-Risk-Analytics-and-Predictive-Modeling/src/utils/data_preparation/feature_engineer.py:126: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy['ClaimPremiumRatio'].replace([np.inf, -np.inf], np.nan, inplace=True) # Modified: `replace` is okay with inplace, but safer to assign\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'ClaimPremiumRatio' feature.\n",
      "Created 'VehicleAge' feature.\n",
      "\n",
      "DataFrame head after Feature Engineering:\n",
      "   UnderwrittenCoverID  PolicyID TransactionMonth  IsVATRegistered  \\\n",
      "0               145249     12827       2015-03-01             True   \n",
      "1               145249     12827       2015-05-01             True   \n",
      "2               145249     12827       2015-07-01             True   \n",
      "3               145255     12827       2015-05-01             True   \n",
      "4               145255     12827       2015-07-01             True   \n",
      "\n",
      "  Citizenship          LegalType Title Language                 Bank  \\\n",
      "0              Close Corporation    Mr  English  First National Bank   \n",
      "1              Close Corporation    Mr  English  First National Bank   \n",
      "2              Close Corporation    Mr  English  First National Bank   \n",
      "3              Close Corporation    Mr  English  First National Bank   \n",
      "4              Close Corporation    Mr  English  First National Bank   \n",
      "\n",
      "       AccountType  ... HasClaim      Margin Month  Year  DayOfWeek DayOfYear  \\\n",
      "0  Current account  ...        0   21.929825     3  2015          6        60   \n",
      "1  Current account  ...        0   21.929825     5  2015          4       121   \n",
      "2  Current account  ...        0    0.000000     7  2015          2       182   \n",
      "3  Current account  ...        0  512.848070     5  2015          4       121   \n",
      "4  Current account  ...        0    0.000000     7  2015          2       182   \n",
      "\n",
      "  WeekOfYear Quarter  ClaimPremiumRatio VehicleAge  \n",
      "0          9       1                0.0         21  \n",
      "1         18       2                0.0         21  \n",
      "2         27       3                0.0         21  \n",
      "3         18       2                0.0         21  \n",
      "4         27       3                0.0         21  \n",
      "\n",
      "[5 rows x 62 columns]\n",
      "\n",
      "DataFrame Info after Feature Engineering:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000098 entries, 0 to 1000097\n",
      "Data columns (total 62 columns):\n",
      " #   Column                    Non-Null Count    Dtype         \n",
      "---  ------                    --------------    -----         \n",
      " 0   UnderwrittenCoverID       1000098 non-null  int64         \n",
      " 1   PolicyID                  1000098 non-null  int64         \n",
      " 2   TransactionMonth          1000098 non-null  datetime64[ns]\n",
      " 3   IsVATRegistered           1000098 non-null  bool          \n",
      " 4   Citizenship               1000098 non-null  object        \n",
      " 5   LegalType                 1000098 non-null  object        \n",
      " 6   Title                     1000098 non-null  object        \n",
      " 7   Language                  1000098 non-null  object        \n",
      " 8   Bank                      854137 non-null   object        \n",
      " 9   AccountType               959866 non-null   object        \n",
      " 10  MaritalStatus             991839 non-null   object        \n",
      " 11  Gender                    990562 non-null   object        \n",
      " 12  Country                   1000098 non-null  object        \n",
      " 13  Province                  1000098 non-null  object        \n",
      " 14  PostalCode                1000098 non-null  int64         \n",
      " 15  MainCrestaZone            1000098 non-null  object        \n",
      " 16  SubCrestaZone             1000098 non-null  object        \n",
      " 17  ItemType                  1000098 non-null  object        \n",
      " 18  mmcode                    999546 non-null   float64       \n",
      " 19  VehicleType               999546 non-null   object        \n",
      " 20  RegistrationYear          1000098 non-null  int64         \n",
      " 21  make                      999546 non-null   object        \n",
      " 22  Model                     999546 non-null   object        \n",
      " 23  Cylinders                 999546 non-null   float64       \n",
      " 24  cubiccapacity             999546 non-null   float64       \n",
      " 25  kilowatts                 999546 non-null   float64       \n",
      " 26  bodytype                  999546 non-null   object        \n",
      " 27  NumberOfDoors             999546 non-null   float64       \n",
      " 28  VehicleIntroDate          999546 non-null   object        \n",
      " 29  CustomValueEstimate       220456 non-null   float64       \n",
      " 30  AlarmImmobiliser          1000098 non-null  object        \n",
      " 31  TrackingDevice            1000098 non-null  object        \n",
      " 32  CapitalOutstanding        1000096 non-null  object        \n",
      " 33  NewVehicle                846803 non-null   object        \n",
      " 34  WrittenOff                358197 non-null   object        \n",
      " 35  Rebuilt                   358197 non-null   object        \n",
      " 36  Converted                 358197 non-null   object        \n",
      " 37  CrossBorder               698 non-null      object        \n",
      " 38  NumberOfVehiclesInFleet   0 non-null        float64       \n",
      " 39  SumInsured                1000098 non-null  float64       \n",
      " 40  TermFrequency             1000098 non-null  object        \n",
      " 41  CalculatedPremiumPerTerm  1000098 non-null  float64       \n",
      " 42  ExcessSelected            1000098 non-null  object        \n",
      " 43  CoverCategory             1000098 non-null  object        \n",
      " 44  CoverType                 1000098 non-null  object        \n",
      " 45  CoverGroup                1000098 non-null  object        \n",
      " 46  Section                   1000098 non-null  object        \n",
      " 47  Product                   1000098 non-null  object        \n",
      " 48  StatutoryClass            1000098 non-null  object        \n",
      " 49  StatutoryRiskType         1000098 non-null  object        \n",
      " 50  TotalPremium              1000098 non-null  float64       \n",
      " 51  TotalClaims               1000098 non-null  float64       \n",
      " 52  HasClaim                  1000098 non-null  int64         \n",
      " 53  Margin                    1000098 non-null  float64       \n",
      " 54  Month                     1000098 non-null  int32         \n",
      " 55  Year                      1000098 non-null  int32         \n",
      " 56  DayOfWeek                 1000098 non-null  int32         \n",
      " 57  DayOfYear                 1000098 non-null  int32         \n",
      " 58  WeekOfYear                1000098 non-null  int64         \n",
      " 59  Quarter                   1000098 non-null  int32         \n",
      " 60  ClaimPremiumRatio         1000098 non-null  float64       \n",
      " 61  VehicleAge                1000098 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), float64(13), int32(5), int64(7), object(35)\n",
      "memory usage: 447.3+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2.2. Feature Engineering ---\")\n",
    "\n",
    "# Create time-based features from 'TransactionMonth'\n",
    "df_fe = create_time_features(df.copy(), 'TransactionMonth')\n",
    "\n",
    "# Create risk ratio features (e.g., ClaimPremiumRatio)\n",
    "df_fe = create_risk_ratio_features(df_fe.copy())\n",
    "\n",
    "# Create vehicle age feature\n",
    "df_fe = create_vehicle_age_feature(df_fe.copy(), current_year=2025) # Adjust current_year as needed\n",
    "\n",
    "print(\"\\nDataFrame head after Feature Engineering:\")\n",
    "print(df_fe.head())\n",
    "print(\"\\nDataFrame Info after Feature Engineering:\")\n",
    "df_fe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36945d",
   "metadata": {},
   "source": [
    "### **2.3. Encoding Categorical Data & Scaling Numerical Data**\n",
    "\n",
    "We'll define the final set of features for our models and then use our `DataPreprocessor` to handle one-hot encoding for categorical features and standard scaling for numerical features. This is crucial as most machine learning models require numerical input and can perform better with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2.3. Encoding Categorical Data & Scaling Numerical Data ---\n",
      "Features selected for preprocessing: 19 numerical, 35 categorical.\n",
      "Numerical Features (after FE and exclusion): ['PostalCode', 'mmcode', 'RegistrationYear', 'Cylinders', 'cubiccapacity', 'kilowatts', 'NumberOfDoors', 'CustomValueEstimate', 'NumberOfVehiclesInFleet', 'SumInsured', 'CalculatedPremiumPerTerm', 'Month', 'Year', 'DayOfWeek', 'DayOfYear', 'WeekOfYear', 'Quarter', 'ClaimPremiumRatio', 'VehicleAge']\n",
      "Categorical Features (after FE and exclusion): ['IsVATRegistered', 'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'VehicleType', 'make', 'Model', 'bodytype', 'AlarmImmobiliser', 'TrackingDevice', 'CapitalOutstanding', 'NewVehicle', 'WrittenOff', 'Rebuilt', 'Converted', 'CrossBorder', 'TermFrequency', 'ExcessSelected', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType']\n",
      "\n",
      "--- Starting Full Preprocessing (Encoder: onehot, Scaler: standard) ---\n",
      "Applying One-Hot Encoding to categorical features...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2.3. Encoding Categorical Data & Scaling Numerical Data ---\")\n",
    "\n",
    "# Define numerical and categorical columns for preprocessing *that will be used as features in models*\n",
    "# Exclude unique identifiers, already processed/target columns, and columns dropped during FE\n",
    "features_to_exclude = [\n",
    "    'PolicyID', 'UnderwrittenCoverID', 'TotalClaims', 'TotalPremium',\n",
    "    'HasClaim', 'Margin', 'TransactionMonth', 'VehicleIntroDate'\n",
    "]\n",
    "\n",
    "# Identify potential numerical and categorical features from the DataFrame after Feature Engineering\n",
    "# Filter to ensure they are present and not in the exclude list\n",
    "all_current_cols = df_fe.columns.tolist()\n",
    "\n",
    "# Attempt to infer data types for better selection, or use a predefined list if schema is known\n",
    "# This list can be more robustly defined based on domain knowledge or a more thorough EDA.\n",
    "inferred_numerical_features = df_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "inferred_categorical_features = df_fe.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "# Filter down to the actual feature columns, excluding targets and IDs\n",
    "final_numerical_features = [\n",
    "    col for col in inferred_numerical_features\n",
    "    if col in all_current_cols and col not in features_to_exclude\n",
    "]\n",
    "final_categorical_features = [\n",
    "    col for col in inferred_categorical_features\n",
    "    if col in all_current_cols and col not in features_to_exclude\n",
    "]\n",
    "\n",
    "# Handle `Mmcode` potentially being treated as categorical or numerical ID.\n",
    "# For modeling, it's often dropped or one-hot encoded if it has few unique values.\n",
    "# Assuming it's an ID for now and thus excluded. If it's a feature, add to numerical/categorical.\n",
    "if 'Mmcode' in final_numerical_features:\n",
    "    final_numerical_features.remove('Mmcode')\n",
    "if 'Mmcode' in final_categorical_features:\n",
    "    final_categorical_features.remove('Mmcode')\n",
    "if 'Mmcode' in features_to_exclude: # Ensure it's in excluded if it was intended to be\n",
    "    pass\n",
    "else:\n",
    "    features_to_exclude.append('Mmcode')\n",
    "\n",
    "\n",
    "print(f\"Features selected for preprocessing: {len(final_numerical_features)} numerical, {len(final_categorical_features)} categorical.\")\n",
    "print(\"Numerical Features (after FE and exclusion):\", final_numerical_features)\n",
    "print(\"Categorical Features (after FE and exclusion):\", final_categorical_features)\n",
    "\n",
    "# Initialize the preprocessor with identified features\n",
    "# For modeling, 'onehot' encoding and 'standard' scaling are good defaults.\n",
    "preprocessor = DataPreprocessor(\n",
    "    numerical_cols=final_numerical_features,\n",
    "    categorical_cols=final_categorical_features\n",
    ")\n",
    "\n",
    "# Apply the full preprocessing pipeline (encoding + scaling)\n",
    "df_processed_for_modeling = preprocessor.preprocess(\n",
    "    df_fe.copy(), # Use a copy of df_fe to avoid modifying it in place if needed later\n",
    "    encoder_type='onehot',\n",
    "    scaler_type='standard'\n",
    ")\n",
    "\n",
    "# Display info about the final processed DataFrame for modeling\n",
    "print(\"\\nDataFrame Info after full preprocessing (for modeling):\")\n",
    "df_processed_for_modeling.info()\n",
    "print(\"\\nTransformed DataFrame Head:\")\n",
    "print(df_processed_for_modeling.head())\n",
    "\n",
    "# Store the final feature names for model interpretability (SHAP/LIME)\n",
    "# The order is important: numerical features first, then one-hot encoded categorical features\n",
    "final_model_feature_names = [col for col in df_processed_for_modeling.columns if col not in features_to_exclude]\n",
    "print(\"\\nFinal feature names for models:\", final_model_feature_names[:10], \"...\") # Print first few\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e05d62",
   "metadata": {},
   "source": [
    "### **2.4. Train-Test Split**\n",
    "\n",
    "We will split the data into training and testing sets. A 70:30 ratio is common. This ensures we evaluate models on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f990ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 2.4. Train-Test Split ---\")\n",
    "\n",
    "# Define features (X) and targets (y) for modeling\n",
    "# X includes all processed features (numerical + one-hot encoded)\n",
    "# y includes the original target columns ('TotalClaims', 'HasClaim', 'TotalPremium')\n",
    "\n",
    "X = df_processed_for_modeling[final_model_feature_names] # Use the identified final feature names\n",
    "y_severity = df_processed_for_modeling['TotalClaims'] # Target for Claim Severity\n",
    "y_probability = df_processed_for_modeling['HasClaim'] # Target for Claim Probability\n",
    "y_premium = df_processed_for_modeling['TotalPremium'] # Target for Direct Premium Prediction\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"TotalClaims target (y_severity) shape: {y_severity.shape}\")\n",
    "print(f\"HasClaim target (y_probability) shape: {y_probability.shape}\")\n",
    "print(f\"TotalPremium target (y_premium) shape: {y_premium.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Use a common random_state for reproducibility\n",
    "X_train, X_test, y_severity_train, y_severity_test = train_test_split(\n",
    "    X, y_severity, test_size=0.3, random_state=42\n",
    ")\n",
    "_, _, y_probability_train, y_probability_test = train_test_split(\n",
    "    X, y_probability, test_size=0.3, random_state=42, stratify=y_probability # Stratify for classification target\n",
    ")\n",
    "_, _, y_premium_train, y_premium_test = train_test_split(\n",
    "    X, y_premium, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nTrain set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"X_train columns (first 5): {X_train.columns.tolist()[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f7728",
   "metadata": {},
   "source": [
    "## **3. Modeling Goal 1: Claim Severity Prediction (Risk Model)**\n",
    "\n",
    "For policies that have a claim (`TotalClaims > 0`), we build a regression model to predict the `TotalClaims` amount. This model is crucial for estimating the financial liability associated with a policy.\n",
    "\n",
    "- **Target Variable**: `TotalClaims` (on the subset of data where claims > 0).\n",
    "- **Evaluation Metric**: Root Mean Squared Error (RMSE) to penalize large prediction errors, and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 3. Modeling Goal 1: Claim Severity Prediction ---\")\n",
    "\n",
    "# Filter data for policies with actual claims (Claim Severity is for WHEN a claim occurs)\n",
    "# Ensure X and y align after filtering\n",
    "claims_mask_train = (y_severity_train > 0)\n",
    "X_train_severity = X_train[claims_mask_train]\n",
    "y_train_severity = y_severity_train[claims_mask_train]\n",
    "\n",
    "claims_mask_test = (y_severity_test > 0)\n",
    "X_test_severity = X_test[claims_mask_test]\n",
    "y_test_severity = y_severity_test[claims_mask_test]\n",
    "\n",
    "print(f\"Claim Severity Training Data Shape (claims > 0): {X_train_severity.shape}, {y_train_severity.shape}\")\n",
    "print(f\"Claim Severity Test Data Shape (claims > 0): {X_test_severity.shape}, {y_test_severity.shape}\")\n",
    "\n",
    "if X_train_severity.empty or X_test_severity.empty:\n",
    "    print(\"Warning: Insufficient data for Claim Severity prediction (very few or no claims with value > 0). Skipping this section.\")\n",
    "else:\n",
    "    # Dictionary to store model evaluation results for comparison\n",
    "    severity_model_results = {}\n",
    "\n",
    "    # Initialize ModelTrainer\n",
    "    trainer = ModelTrainer(LinearRegressionStrategy()) # Start with LR\n",
    "\n",
    "    # --- 3.1. Model Building & Training ---\n",
    "\n",
    "    # Model 1: Linear Regression for Claim Severity\n",
    "    print(\"\\n--- Training Linear Regression for Claim Severity ---\")\n",
    "    trainer.train_model(X_train_severity, y_train_severity)\n",
    "    lr_severity_predictions = trainer.predict_model(X_test_severity)\n",
    "    lr_severity_model = trainer.get_current_model_object()\n",
    "    severity_model_results['Linear Regression'] = evaluate_regression_model(y_test_severity, lr_severity_predictions)\n",
    "\n",
    "\n",
    "    # Model 2: Decision Tree for Claim Severity \n",
    "    print(\"\\n--- Training Decision Tree Regressor for Claim Severity ---\")\n",
    "    trainer.set_strategy(DecisionTreeStrategy(model_type='regressor', random_state=42))\n",
    "    trainer.train_model(X_train_severity, y_train_severity)\n",
    "    dt_severity_predictions = trainer.predict_model(X_test_severity)\n",
    "    dt_severity_model = trainer.get_current_model_object()\n",
    "    severity_model_results['Decision Tree'] = evaluate_regression_model(y_test_severity, dt_severity_predictions)\n",
    "\n",
    "\n",
    "    # Model 3: Random Forest for Claim Severity\n",
    "    print(\"\\n--- Training Random Forest for Claim Severity ---\")\n",
    "    trainer.set_strategy(RandomForestStrategy(n_estimators=200, random_state=42))\n",
    "    trainer.train_model(X_train_severity, y_train_severity)\n",
    "    rf_severity_predictions = trainer.predict_model(X_test_severity)\n",
    "    rf_severity_model = trainer.get_current_model_object()\n",
    "    severity_model_results['Random Forest'] = evaluate_regression_model(y_test_severity, rf_severity_predictions)\n",
    "\n",
    "\n",
    "    # Model 4: XGBoost for Claim Severity\n",
    "    print(\"\\n--- Training XGBoost Regressor for Claim Severity ---\")\n",
    "    trainer.set_strategy(XGBoostStrategy(objective='reg:squarederror', n_estimators=200, random_state=42))\n",
    "    trainer.train_model(X_train_severity, y_train_severity)\n",
    "    xgb_severity_predictions = trainer.predict_model(X_test_severity)\n",
    "    xgb_severity_model = trainer.get_current_model_object()\n",
    "    severity_model_results['XGBoost Regressor'] = evaluate_regression_model(y_test_severity, xgb_severity_predictions)\n",
    "\n",
    "\n",
    "    # --- 3.2. Model Evaluation (Consolidated) ---\n",
    "    print(\"\\n--- Claim Severity Model Comparison ---\")\n",
    "    severity_comparison_df = pd.DataFrame(severity_model_results).T\n",
    "    print(severity_comparison_df.sort_values(by='RMSE')) # Sort by RMSE to find best performing\n",
    "\n",
    "    # Identify the best performing model based on RMSE\n",
    "    best_severity_model_name = severity_comparison_df['RMSE'].idxmin()\n",
    "    print(f\"\\nBest performing Claim Severity model based on RMSE: {best_severity_model_name}\")\n",
    "\n",
    "    # Select the best model object for interpretability\n",
    "    best_severity_model = {\n",
    "        'Linear Regression': lr_severity_model,\n",
    "        'Decision Tree': dt_severity_model,\n",
    "        'Random Forest': rf_severity_model,\n",
    "        'XGBoost Regressor': xgb_severity_model\n",
    "    }.get(best_severity_model_name)\n",
    "\n",
    "    # --- 3.3. Model Interpretability (SHAP & LIME) ---\n",
    "    if best_severity_model and not X_test_severity.empty:\n",
    "        print(f\"\\n--- Model Interpretability for {best_severity_model_name} (Claim Severity) ---\")\n",
    "\n",
    "        # Initialize ModelInterpreter with the best model and features for SHAP/LIME\n",
    "        # Ensure feature_names correspond to X_test_severity columns\n",
    "        interpreter_severity = ModelInterpreter(\n",
    "            model=best_severity_model,\n",
    "            feature_names=X_test_severity.columns.tolist(),\n",
    "            model_type='regression'\n",
    "        )\n",
    "\n",
    "        # SHAP for overall feature importance\n",
    "        # Use a subset of X_test_severity for SHAP explanation if dataset is large,\n",
    "        # as KernelExplainer can be computationally intensive. TreeExplainer is faster.\n",
    "        shap_X_severity = X_test_severity.sample(min(1000, len(X_test_severity)), random_state=42) if len(X_test_severity) > 1000 else X_test_severity\n",
    "\n",
    "        interpreter_severity.explain_model_shap(shap_X_severity)\n",
    "        print(\"\\nSHAP Summary Plot (Global Feature Importance for Claim Severity):\")\n",
    "        interpreter_severity.plot_shap_summary(shap_X_severity)\n",
    "\n",
    "        # LIME for individual instance explanation\n",
    "        print(\"\\nLIME Explanation for a Sample Prediction (first instance in test set):\")\n",
    "        # For LIME explainer, it's best to initialize once with representative training data\n",
    "        interpreter_severity.lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "            training_data=X_train_severity.values,\n",
    "            feature_names=X_train_severity.columns.tolist(),\n",
    "            mode='regression'\n",
    "        )\n",
    "        # Explain the first instance from the test set\n",
    "        interpreter_severity.explain_instance_lime(X_test_severity.iloc[0])\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping model interpretability for Claim Severity due to insufficient data or no best model found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54197274",
   "metadata": {},
   "source": [
    "## **4. Modeling Goal 2: Premium Optimization (Pricing Framework)**\n",
    "\n",
    "The ultimate goal is a dynamic, risk-based pricing system. This involves predicting the probability of a claim and its severity.\n",
    "\n",
    "### **4.1. Conceptual Framework: Risk-Based Premium**\n",
    "\n",
    "The prompt outlines a sophisticated approach for a Risk-Based Premium:\n",
    "\n",
    "Premium = (Predicted Probability of Claim * Predicted Claim Severity) + Expense Loading + Profit Margin\n",
    "\n",
    "This formula suggests a two-stage modeling approach:\n",
    "\n",
    "1. **Probability of Claim Model**: A classification model predicts `P(Claim = 1)`.\n",
    "2. **Claim Severity Model**: A regression model (which we just built) predicts `E[TotalClaims | Claim = 1]`.\n",
    "\n",
    "We then combine these with business-defined \"Expense Loading\" (costs of operations, sales, etc.) and \"Profit Margin\" to arrive at a recommended premium.\n",
    "\n",
    "### **4.2. Model Building: Probability of Claim (Classification Model)**\n",
    "\n",
    "We will build a binary classification model to predict `HasClaim` (0 or 1).\n",
    "\n",
    "- **Target Variable**: `HasClaim`.\n",
    "- **Evaluation Metric**: Accuracy, Precision, Recall, F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 4.2. Model Building: Probability of Claim (Classification Model) ---\")\n",
    "\n",
    "# For classification, we use y_probability_train and y_probability_test\n",
    "# Ensure that the target column 'HasClaim' is suitable for classification (binary: 0 or 1)\n",
    "print(f\"Claim Probability Training Target Value Counts:\\n{y_probability_train.value_counts()}\")\n",
    "print(f\"Claim Probability Test Target Value Counts:\\n{y_probability_test.value_counts()}\")\n",
    "\n",
    "# Check for class imbalance which is common in claim data\n",
    "if not y_probability_train.empty and y_probability_train.value_counts().min() / y_probability_train.value_counts().max() < 0.2:\n",
    "    print(\"Warning: Significant class imbalance detected in 'HasClaim'. Consider techniques like SMOTE or class weighting for training.\")\n",
    "else:\n",
    "    print(\"Class distribution for 'HasClaim' appears balanced enough or data is too small to assess imbalance.\")\n",
    "\n",
    "# Dictionary to store classification model evaluation results\n",
    "probability_model_results = {}\n",
    "\n",
    "# Initialize ModelTrainer\n",
    "trainer_clf = ModelTrainer(DecisionTreeStrategy(model_type='classifier', random_state=42)) # Start with DT Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d901",
   "metadata": {},
   "source": [
    "### Model 1: Decision Tree Classifier for Claim Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Decision Tree Classifier for Claim Probability \n",
    "print(\"\\n--- Training Decision Tree Classifier for Claim Probability ---\")\n",
    "trainer_clf.train_model(X_train, y_probability_train)\n",
    "dt_probability_predictions_proba = trainer_clf.predict_model(X_test)\n",
    "dt_probability_model = trainer_clf.get_current_model_object()\n",
    "probability_model_results['Decision Tree Classifier'] = evaluate_classification_model(y_probability_test, dt_probability_predictions_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad6497",
   "metadata": {},
   "source": [
    "### Model 2: XGBoost Classifier for Claim Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: XGBoost Classifier for Claim Probability\n",
    "print(\"\\n--- Training XGBoost Classifier for Claim Probability ---\")\n",
    "# Use use_label_encoder=False and eval_metric for compatibility with latest XGBoost\n",
    "trainer_clf.set_strategy(XGBoostStrategy(objective='binary:logistic', n_estimators=200, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "trainer_clf.train_model(X_train, y_probability_train) # Using full X_train for probability model\n",
    "xgb_probability_predictions_proba = trainer_clf.predict_model(X_test) # Get probabilities\n",
    "xgb_probability_model = trainer_clf.get_current_model_object()\n",
    "probability_model_results['XGBoost Classifier'] = evaluate_classification_model(y_probability_test, xgb_probability_predictions_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25b829",
   "metadata": {},
   "source": [
    "### **4.3. Model Evaluation: Probability of Claim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3. Model Evaluation: Probability of ClaimModel Evaluation: Probability of Claim (Consolidated) ---\n",
    "print(\"\\n--- Claim Probability Model Performance Summary ---\")\n",
    "probability_comparison_df = pd.DataFrame(probability_model_results).T\n",
    "print(probability_comparison_df.sort_values(by='F1-score', ascending=False)) # Sort by F1-score for classification\n",
    "\n",
    "# Identify the best performing model based on F1-score\n",
    "best_probability_model_name = probability_comparison_df['F1-score'].idxmax()\n",
    "print(f\"\\nBest performing Claim Probability model based on F1-score: {best_probability_model_name}\")\n",
    "\n",
    "# Select the best model object\n",
    "best_probability_model = {\n",
    "    'Decision Tree Classifier': dt_probability_model,\n",
    "    'XGBoost Classifier': xgb_probability_model\n",
    "}.get(best_probability_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2153c",
   "metadata": {},
   "source": [
    "### **4.4. Model Interpretability (SHAP & LIME) for Probability Model**\n",
    "\n",
    "Understanding which features drive the prediction of a claim occurring is vital for risk assessment and underwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_probability_model and not X_test.empty:\n",
    "    print(f\"\\n--- Model Interpretability for {best_probability_model_name} (Claim Probability) ---\")\n",
    "\n",
    "    # Initialize ModelInterpreter for classification\n",
    "    # Ensure class_names are correct for your binary target (e.g., [0, 1] or ['No Claim', 'Claim'])\n",
    "    interpreter_prob = ModelInterpreter(\n",
    "        model=best_probability_model,\n",
    "        feature_names=X_test.columns.tolist(),\n",
    "        class_names=['No Claim', 'Claim'], # Assuming 0 for No Claim, 1 for Claim\n",
    "        model_type='classification'\n",
    "    )\n",
    "\n",
    "    # SHAP for overall feature importance\n",
    "    shap_X_prob = X_test.sample(min(1000, len(X_test)), random_state=42) if len(X_test) > 1000 else X_test\n",
    "    interpreter_prob.explain_model_shap(shap_X_prob)\n",
    "    print(\"\\nSHAP Summary Plot (Global Feature Importance for Claim Probability):\")\n",
    "    interpreter_prob.plot_shap_summary(shap_X_prob)\n",
    "\n",
    "    # LIME for individual instance explanation\n",
    "    print(\"\\nLIME Explanation for a Sample Probability Prediction (first instance in test set):\")\n",
    "    interpreter_prob.lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train.values, # LIME needs the raw training data array\n",
    "        feature_names=X_train.columns.tolist(),\n",
    "        class_names=['No Claim', 'Claim'],\n",
    "        mode='classification'\n",
    "    )\n",
    "    interpreter_prob.explain_instance_lime(X_test.iloc[0])\n",
    "else:\n",
    "    print(\"Skipping model interpretability for Claim Probability due to insufficient data or no best model found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5cf24d",
   "metadata": {},
   "source": [
    "### **4.5. Simplified Premium Prediction (Direct Regression on TotalPremium)**\n",
    "\n",
    "While the full \"Risk-Based Premium\" formula involves combining models, a simpler approach for \"predicting an appropriate premium\" might be to directly regress on `TotalPremium`. This demonstrates a pricing model without the full two-stage complexity.\n",
    "\n",
    "- **Target Variable**: `TotalPremium`.\n",
    "- **Evaluation Metric**: RMSE, R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 4.5. Simplified Premium Prediction (Direct Regression on TotalPremium) ---\")\n",
    "\n",
    "# Dictionary to store direct premium model evaluation results\n",
    "premium_model_results = {}\n",
    "\n",
    "# Initialize ModelTrainer (using full X_train for premium prediction)\n",
    "trainer_premium = ModelTrainer(LinearRegressionStrategy()) # Start with LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68278846",
   "metadata": {},
   "source": [
    "### Model 1: Linear Regression for TotalPremium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8958ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Linear Regression for TotalPremium\n",
    "print(\"\\n--- Training Linear Regression for TotalPremium ---\")\n",
    "trainer_premium.train_model(X_train, y_premium_train)\n",
    "lr_premium_predictions = trainer_premium.predict_model(X_test)\n",
    "lr_premium_model = trainer_premium.get_current_model_object()\n",
    "premium_model_results['Linear Regression'] = evaluate_regression_model(y_premium_test, lr_premium_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f3d01",
   "metadata": {},
   "source": [
    "### Model 2: Decision Tree for TotalPremium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Decision Tree for TotalPremium \n",
    "print(\"\\n--- Training Decision Tree Regressor for TotalPremium ---\")\n",
    "trainer_premium.set_strategy(DecisionTreeStrategy(model_type='regressor', random_state=42))\n",
    "trainer_premium.train_model(X_train, y_premium_train)\n",
    "dt_premium_predictions = trainer_premium.predict_model(X_test)\n",
    "dt_premium_model = trainer_premium.get_current_model_object()\n",
    "premium_model_results['Decision Tree'] = evaluate_regression_model(y_premium_test, dt_premium_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7d52e",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest for TotalPremium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest for TotalPremium\n",
    "print(\"\\n--- Training Random Forest for TotalPremium ---\")\n",
    "trainer_premium.set_strategy(RandomForestStrategy(n_estimators=200, random_state=42))\n",
    "trainer_premium.train_model(X_train, y_premium_train)\n",
    "rf_premium_predictions = trainer_premium.predict_model(X_test)\n",
    "rf_premium_model = trainer_premium.get_current_model_object()\n",
    "premium_model_results['Random Forest'] = evaluate_regression_model(y_premium_test, rf_premium_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c412213f",
   "metadata": {},
   "source": [
    "### Model 4: XGBoost for TotalPremium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ba66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: XGBoost for TotalPremium\n",
    "print(\"\\n--- Training XGBoost Regressor for TotalPremium ---\")\n",
    "trainer_premium.set_strategy(XGBoostStrategy(objective='reg:squarederror', n_estimators=200, random_state=42))\n",
    "trainer_premium.train_model(X_train, y_premium_train)\n",
    "xgb_premium_predictions = trainer_premium.predict_model(X_test)\n",
    "xgb_premium_model = trainer_premium.get_current_model_object()\n",
    "premium_model_results['XGBoost Regressor'] = evaluate_regression_model(y_premium_test, xgb_premium_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba345b0c",
   "metadata": {},
   "source": [
    "### Direct Premium Prediction Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Direct Premium Prediction Model Comparison ---\")\n",
    "premium_comparison_df = pd.DataFrame(premium_model_results).T\n",
    "print(premium_comparison_df.sort_values(by='RMSE'))\n",
    "\n",
    "# Identify the best performing model based on RMSE\n",
    "best_premium_model_name = premium_comparison_df['RMSE'].idxmin()\n",
    "print(f\"\\nBest performing Direct Premium Prediction model based on RMSE: {best_premium_model_name}\")\n",
    "\n",
    "# Select the best model object for interpretability\n",
    "best_premium_model = {\n",
    "    'Linear Regression': lr_premium_model,\n",
    "    'Decision Tree': dt_premium_model,\n",
    "    'Random Forest': rf_premium_model,\n",
    "    'XGBoost Regressor': xgb_premium_model\n",
    "}.get(best_premium_model_name)\n",
    "\n",
    "# Feature Importance for the best direct premium prediction model\n",
    "if best_premium_model and not X_test.empty:\n",
    "    print(f\"\\n--- Model Interpretability for {best_premium_model_name} (Direct Premium Prediction) ---\")\n",
    "    interpreter_premium = ModelInterpreter(\n",
    "        model=best_premium_model,\n",
    "        feature_names=X_test.columns.tolist(),\n",
    "        model_type='regression'\n",
    "    )\n",
    "\n",
    "    shap_X_premium = X_test.sample(min(1000, len(X_test)), random_state=42) if len(X_test) > 1000 else X_test\n",
    "\n",
    "    interpreter_premium.explain_model_shap(shap_X_premium)\n",
    "    print(\"\\nSHAP Summary Plot (Global Feature Importance for Direct Premium Prediction):\")\n",
    "    interpreter_premium.plot_shap_summary(shap_X_premium)\n",
    "\n",
    "    print(\"\\nLIME Explanation for a Sample Premium Prediction (first test instance):\")\n",
    "    interpreter_premium.lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train.values,\n",
    "        feature_names=X_train.columns.tolist(),\n",
    "        mode='regression'\n",
    "    )\n",
    "    interpreter_premium.explain_instance_lime(X_test.iloc[0])\n",
    "else:\n",
    "    print(\"Skipping model interpretability for Direct Premium Prediction due to insufficient data or no best model found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee487e74",
   "metadata": {},
   "source": [
    "## **5. Overall Model Comparison and Interpretation**\n",
    "\n",
    "Based on the evaluation metrics and feature importance analysis, compare the performance of the developed models for both Claim Severity and Premium Prediction.\n",
    "\n",
    "*(**Your detailed interpretation and comparison here, based on the outputs from the cells above**)*\n",
    "\n",
    "- **Claim Severity Model Comparison:**\n",
    "    - **RMSE & R-squared**: Which model minimized RMSE and maximized R-squared on the `claims > 0` subset? This indicates its accuracy in predicting the actual cost of a claim.\n",
    "    - **Feature Importance (SHAP & LIME)**: Identify the top 5-10 most influential features for the best Claim Severity model. Explain their impact (e.g., \"SHAP analysis reveals that for every year older a vehicle is, the predicted claim amount increases by X Rand, holding other factors constant. This provides quantitative evidence to refine our age-based premium adjustments.\"). How do global (SHAP) and local (LIME) explanations compare?\n",
    "- **Premium Probability (Classification) Model Comparison:**\n",
    "    - **Metrics**: Which model performed best on Accuracy, Precision, Recall, and F1-score for predicting `HasClaim`? Consider the business context: is it more critical to identify all potential claims (high recall) or to only identify claims with high certainty (high precision)?\n",
    "    - **Feature Importance (SHAP & LIME)**: Identify the top 5-10 most influential features for the Claim Probability model. Explain their impact on the likelihood of a claim. How do global (SHAP) and local (LIME) explanations compare?\n",
    "- **Direct Premium Prediction Model Comparison:**\n",
    "    - **RMSE & R-squared**: Which model best predicted `TotalPremium`?\n",
    "    - **Feature Importance (SHAP & LIME)**: Identify the top 5-10 features for predicting the premium. This can reveal what attributes are currently strongly correlated with the premiums being charged. How do global (SHAP) and local (LIME) explanations compare?\n",
    "\n",
    "**Interpretation Insights:**\n",
    "\n",
    "- **Overlap in Important Features**: Are there common influential features across Claim Severity, Claim Probability, and Direct Premium Prediction models? These are likely the most critical risk drivers.\n",
    "- **Discrepancies**: If current `TotalPremium` drivers differ significantly from actual `TotalClaims` or `HasClaim` drivers, it highlights potential mispricing opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c090536",
   "metadata": {},
   "source": [
    "## **6. Conclusion and Business Recommendations**\n",
    "\n",
    "Synthesize all findings from model building and evaluation into actionable business recommendations for ACIS's dynamic, risk-based pricing system.\n",
    "\n",
    "**Key Model Findings Summary:**\n",
    "\n",
    "- **Claim Severity Model**: [Best Model Name] achieved [RMSE value] and [R-squared value]. The most influential features were [list top 3-5, e.g., 'VehicleAge', 'CustomValueEstimate', 'Make'].\n",
    "    - *Interpretation*: Provide 1-2 sentences on what these features imply for financial liability (e.g., \"Vehicle age and custom value estimate are significant drivers of claim severity, indicating older, more valuable vehicles accrue higher claim costs. For example, LIME analysis showed that for a specific high-claim policy, a high custom value was a strong positive contributor to the predicted severity.\").\n",
    "- **Claim Probability Model**: [Best Model Name] achieved [Accuracy/F1-score]. The most influential features were [list top 3-5, e.g., 'Province_Gauteng', 'VehicleType_SUV', 'PostalCode_2000'].\n",
    "    - *Interpretation*: Provide 1-2 sentences on what these features imply for claim likelihood (e.g., \"Certain vehicle types and postal codes significantly influence the probability of a claim, suggesting localized risk factors. For instance, LIME highlights that living in 'Gauteng' strongly increases the predicted probability of a claim for an individual policy.\").\n",
    "- **Direct Premium Model**: [Best Model Name] achieved [RMSE value] and [R-squared value]. The most influential features were [list top 3-5, e.g., 'SumInsured', 'CoverType', 'TermFrequency'].\n",
    "    - *Interpretation*: Provide 1-2 sentences on what these features indicate about current pricing (e.g., \"Current premiums are heavily influenced by sum insured and cover type, but perhaps less directly by actual claims history or vehicle age, which could be an area for refinement. SHAP values confirm that a higher 'SumInsured' directly leads to a higher predicted premium.\").\n",
    "\n",
    "**Strategic Recommendations for Dynamic, Risk-Based Pricing:**\n",
    "\n",
    "1. **Develop a Two-Stage Risk-Based Premium Model**: Implement the conceptual framework: `Premium = (Predicted Probability of Claim * Predicted Claim Severity) + Expense Loading + Profit Margin`. This requires deploying both the best-performing Claim Probability (classification) model and Claim Severity (regression) model. This sophisticated approach will ensure premiums are directly tied to the expected loss for each policy.\n",
    "2. **Refine Feature Engineering**: Leverage the identified feature importance from SHAP and LIME analysis. Focus on collecting and utilizing data for the most influential features. For instance, if 'VehicleAge' is highly important for severity, ensure this is accurately captured and used in pricing algorithms.\n",
    "3. **Optimize Pricing Segments**: The models provide a granular understanding of risk drivers. ACIS can now move beyond broad segments to dynamic pricing that incorporates multiple predictive features. For example, rather than just province, pricing can factor in `VehicleAge`, `CustomValueEstimate`, and `PostalCode` simultaneously, weighted by their predictive power.\n",
    "4. **Continuous Model Monitoring & Retraining**: Predictive models degrade over time. Implement a robust MLOps pipeline to continuously monitor model performance (RMSE, R-squared, classification metrics) and retrain models with fresh data to adapt to changing market conditions and claim patterns. Specifically, monitor model drift for key features identified by SHAP/LIME.\n",
    "5. **Integration with Underwriting**: The feature importance insights can directly inform underwriting rules, allowing for automated decision-making or flagging of high-risk policies that require manual review. LIME can be particularly useful for underwriters to understand *why* a specific policy received a certain risk score.\n",
    "6. **Business Buy-in and Explainability**: Use SHAP visualizations and quantitative explanations (like \"for every year older a vehicle is...\") to communicate model insights to business stakeholders, fostering trust and enabling data-driven decisions. LIME provides policy-specific explanations, which can be invaluable for customer service or claims dispute resolution.\n",
    "\n",
    "By implementing these recommendations, ACIS can transition to a more data-driven, precise, and dynamic risk-based pricing system, leading to improved profitability, fairer premiums for customers, and a competitive advantage in the insurance market."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
